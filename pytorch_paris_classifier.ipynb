{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1723/2851567373.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd \n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 8000\n",
      "Eval set size: 2000\n"
     ]
    }
   ],
   "source": [
    "# df = \n",
    "# shape = df.shape\n",
    "# print(shape)\n",
    "# print(df.head())\n",
    "\n",
    "# training, test = torch.utils.data.random_split(df, [int(shape[0]*0.8), int(shape[0]*0.2)])\n",
    "training = pd.read_csv(\"data/ParisHousingClass.csv\", sep=\",\", header=\"infer\")\n",
    "test = pd.read_csv(\"data/ParisHousingClass_Test.csv\", sep=\",\", header=\"infer\")\n",
    "features = training.shape[1] - 1\n",
    "\n",
    "print(f\"Training set size: {len(training)}\")\n",
    "print(f\"Eval set size: {len(test)}\")\n",
    "\n",
    "categories = training['category'].unique()\n",
    "category_mapping = {cat: i for i, cat in enumerate(categories)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParisCategoryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParisCategoryClassifier(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "(8000, 18)\n",
      "(8000, 18)\n",
      "(8000, 18)\n",
      "Training set has 8000 instances\n",
      "(2000, 18)\n",
      "Validation set has 2000 instances\n"
     ]
    }
   ],
   "source": [
    "model = ParisCategoryClassifier().to(device)\n",
    "print(model)\n",
    "\n",
    "class ParisDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        print(self.df.shape)\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.drop(columns=['category']).iloc[idx]\n",
    "        label = self.df.iloc[idx]['category']\n",
    "        # print(f\"Label {label}\")\n",
    "        label = category_mapping[label]\n",
    "        # print(f\"Label after mapping {label}\")\n",
    "        # print(f\"Data values {data.values}\")\n",
    "\n",
    "        data_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "        # print(f\"Data tensor {data_tensor}\")\n",
    "        label_tensor = torch.tensor(label, dtype=torch.int64)\n",
    "        # print(f\"Label tensor {label_tensor}\")\n",
    "        \n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "training_set = ParisDataset(training)\n",
    "test_set = ParisDataset(test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False)\n",
    "\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(test_set)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 18)\n",
      "(8000, 18)\n",
      "  batch 1000 loss: nan\n",
      "(8000, 18)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'add_scalar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m             running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_loss\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 35\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, last_loss))\n\u001b[1;32m     34\u001b[0m         tb_x \u001b[38;5;241m=\u001b[39m epoch_index \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(training_loader) \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 35\u001b[0m         \u001b[43mtb_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m'\u001b[39m, last_loss, tb_x)\n\u001b[1;32m     36\u001b[0m         running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m last_loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'add_scalar'"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        # print(f\"{outputs.shape} {labels.shape}\")\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "train_one_epoch(0, None)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
